{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn0G2rHZwZtm"
      },
      "outputs": [],
      "source": [
        "!pip install noisereduce librosa pandas numpy h5py tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bq6qw9t2uDh",
        "outputId": "d8ff9372-057d-4735-9b97-6e8f8600e740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxvNXdscw7hV"
      },
      "source": [
        "# Data Gathering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11S31rY1xCWx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import noisereduce as nr\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import multiprocessing\n",
        "\n",
        "class AudioBulkProcessor:\n",
        "    \"\"\"\n",
        "    Process bulk audio files with normalization, noise reduction, and silence removal\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target_sr=16000, n_jobs=-1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            target_sr: Target sample rate for all audio\n",
        "            n_jobs: Number of parallel jobs (-1 for all cores)\n",
        "        \"\"\"\n",
        "        self.target_sr = target_sr\n",
        "        self.n_jobs = multiprocessing.cpu_count() if n_jobs == -1 else n_jobs\n",
        "\n",
        "    def process_file(self, input_path, output_path=None,\n",
        "                    normalize=True, noise_reduce=True,\n",
        "                    remove_silence=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Process single audio file\n",
        "\n",
        "        Args:\n",
        "            input_path: Path to input audio file\n",
        "            output_path: Path to save processed audio (None for in-place)\n",
        "            normalize: Apply normalization\n",
        "            noise_reduce: Apply noise reduction\n",
        "            remove_silence: Remove leading/trailing silence\n",
        "            **kwargs: Additional parameters for each processing step\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio, sr = librosa.load(input_path, sr=None)\n",
        "\n",
        "            # Resample if needed\n",
        "            if sr != self.target_sr:\n",
        "                audio = librosa.resample(y=audio, orig_sr=sr, target_sr=self.target_sr)\n",
        "                sr = self.target_sr\n",
        "\n",
        "            # Apply processing steps\n",
        "            if remove_silence:\n",
        "                audio = self._remove_silence(audio, sr, **kwargs.get('silence_params', {}))\n",
        "\n",
        "            if noise_reduce:\n",
        "                audio = self._reduce_noise(audio, sr, **kwargs.get('noise_params', {}))\n",
        "\n",
        "            if normalize:\n",
        "                audio = self._normalize_audio(audio, **kwargs.get('norm_params', {}))\n",
        "\n",
        "            # Determine output path\n",
        "            if output_path is None:\n",
        "                output_path = input_path  # Overwrite original\n",
        "\n",
        "            # Save processed audio\n",
        "            sf.write(output_path, audio, sr)\n",
        "\n",
        "            return True, output_path, len(audio)/sr\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, str(e), 0\n",
        "\n",
        "    def process_directory(self, input_dir, output_dir=None,\n",
        "                         file_extensions=('.wav', '.mp3', '.flac'),\n",
        "                         **process_kwargs):\n",
        "        \"\"\"\n",
        "        Process all audio files in a directory\n",
        "\n",
        "        Args:\n",
        "            input_dir: Input directory path\n",
        "            output_dir: Output directory path (None for in-place)\n",
        "            file_extensions: Tuple of file extensions to process\n",
        "            **process_kwargs: Parameters for process_file method\n",
        "        \"\"\"\n",
        "        # Get all audio files\n",
        "        input_paths = []\n",
        "        for ext in file_extensions:\n",
        "            input_paths.extend(Path(input_dir).rglob(f'*{ext}'))\n",
        "            input_paths.extend(Path(input_dir).rglob(f'*{ext.upper()}'))\n",
        "\n",
        "        input_paths = [str(p) for p in input_paths]\n",
        "\n",
        "        print(f\"Found {len(input_paths)} audio files to process\")\n",
        "\n",
        "        # Create output directory if specified\n",
        "        if output_dir:\n",
        "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Process files\n",
        "        results = []\n",
        "\n",
        "        if self.n_jobs > 1:\n",
        "            # Parallel processing\n",
        "            with ProcessPoolExecutor(max_workers=self.n_jobs) as executor:\n",
        "                futures = []\n",
        "                for input_path in input_paths:\n",
        "                    # Determine output path\n",
        "                    if output_dir:\n",
        "                        rel_path = Path(input_path).relative_to(input_dir)\n",
        "                        output_path = Path(output_dir) / rel_path\n",
        "                        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                        output_path = str(output_path)\n",
        "                    else:\n",
        "                        output_path = input_path\n",
        "\n",
        "                    futures.append(\n",
        "                        executor.submit(\n",
        "                            self._process_file_wrapper,\n",
        "                            input_path, output_path, **process_kwargs\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # Collect results with progress bar\n",
        "                for future in tqdm(futures, desc=\"Processing audio files\"):\n",
        "                    results.append(future.result())\n",
        "        else:\n",
        "            # Sequential processing\n",
        "            for input_path in tqdm(input_paths, desc=\"Processing audio files\"):\n",
        "                if output_dir:\n",
        "                    rel_path = Path(input_path).relative_to(input_dir)\n",
        "                    output_path = Path(output_dir) / rel_path\n",
        "                    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                    output_path = str(output_path)\n",
        "                else:\n",
        "                    output_path = input_path\n",
        "\n",
        "                result = self._process_file_wrapper(input_path, output_path, **process_kwargs)\n",
        "                results.append(result)\n",
        "\n",
        "        # Summary statistics\n",
        "        successful = [r for r in results if r[0]]\n",
        "        failed = [r for r in results if not r[0]]\n",
        "\n",
        "        print(f\"\\nProcessing Complete:\")\n",
        "        print(f\"  Successful: {len(successful)} files\")\n",
        "        print(f\"  Failed: {len(failed)} files\")\n",
        "\n",
        "        if failed:\n",
        "            print(\"\\nFailed files:\")\n",
        "            for failure in failed[:10]:  # Show first 10 failures\n",
        "                print(f\"  {failure[1]}\")\n",
        "            if len(failed) > 10:\n",
        "                print(f\"  ... and {len(failed)-10} more\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _process_file_wrapper(self, *args, **kwargs):\n",
        "        \"\"\"Wrapper for parallel processing\"\"\"\n",
        "        return self.process_file(*args, **kwargs)\n",
        "\n",
        "    # Processing methods\n",
        "    def _normalize_audio(self, audio, method='peak', target_level=-1.0,\n",
        "                        loudness_level=-23.0):\n",
        "        \"\"\"\n",
        "        Normalize audio\n",
        "\n",
        "        Args:\n",
        "            audio: Input audio array\n",
        "            method: 'peak', 'rms', or 'loudness'\n",
        "            target_level: Target peak level in dB (for peak normalization)\n",
        "            loudness_level: Target LUFS level (for loudness normalization)\n",
        "        \"\"\"\n",
        "        if method == 'peak':\n",
        "            # Peak normalization to target level\n",
        "            peak = np.max(np.abs(audio))\n",
        "            if peak > 0:\n",
        "                target_linear = 10**(target_level / 20)\n",
        "                audio = audio * (target_linear / peak)\n",
        "\n",
        "        elif method == 'rms':\n",
        "            # RMS normalization\n",
        "            rms = np.sqrt(np.mean(audio**2))\n",
        "            target_rms = 10**(target_level / 20)\n",
        "            if rms > 0:\n",
        "                audio = audio * (target_rms / rms)\n",
        "\n",
        "        elif method == 'loudness':\n",
        "            # Loudness normalization (requires pydub)\n",
        "            try:\n",
        "                from pydub import AudioSegment\n",
        "                from pydub.effects import normalize\n",
        "                import io\n",
        "\n",
        "                # Convert numpy array to AudioSegment\n",
        "                audio_segment = AudioSegment(\n",
        "                    audio.tobytes(),\n",
        "                    frame_rate=self.target_sr,\n",
        "                    sample_width=audio.dtype.itemsize,\n",
        "                    channels=1\n",
        "                )\n",
        "\n",
        "                # Apply loudness normalization\n",
        "                audio_segment = normalize(audio_segment, headroom=0.1)\n",
        "\n",
        "                # Convert back to numpy\n",
        "                audio = np.array(audio_segment.get_array_of_data())\n",
        "                audio = audio.astype(np.float32) / (2**15)  # For 16-bit audio\n",
        "\n",
        "            except ImportError:\n",
        "                print(\"Warning: pydub not installed, using peak normalization instead\")\n",
        "                peak = np.max(np.abs(audio))\n",
        "                if peak > 0:\n",
        "                    audio = audio / peak\n",
        "\n",
        "        # Apply safety ceiling\n",
        "        audio = np.clip(audio, -0.99, 0.99)\n",
        "\n",
        "        return audio\n",
        "\n",
        "    def _reduce_noise(self, audio, sr, method='spectral_gate',\n",
        "                     stationary=True, prop_decrease=1.0,\n",
        "                     freq_mask_smooth_hz=500, time_mask_smooth_ms=50):\n",
        "        \"\"\"\n",
        "        Reduce noise in audio\n",
        "\n",
        "        Args:\n",
        "            audio: Input audio array\n",
        "            sr: Sample rate\n",
        "            method: 'spectral_gate', 'wiener', or 'none'\n",
        "            stationary: Whether noise is stationary\n",
        "            prop_decrease: Proportion of noise to decrease (0-1)\n",
        "        \"\"\"\n",
        "        if method == 'spectral_gate':\n",
        "            # Spectral subtraction using noisereduce\n",
        "            try:\n",
        "                # Estimate noise from first 500ms\n",
        "                noise_sample = audio[:int(0.5 * sr)]\n",
        "\n",
        "                audio_clean = nr.reduce_noise(\n",
        "                    y=audio,\n",
        "                    sr=sr,\n",
        "                    y_noise=noise_sample,\n",
        "                    stationary=stationary,\n",
        "                    prop_decrease=prop_decrease,\n",
        "                    freq_mask_smooth_hz=freq_mask_smooth_hz,\n",
        "                    time_mask_smooth_ms=time_mask_smooth_ms\n",
        "                )\n",
        "                return audio_clean\n",
        "            except:\n",
        "                return audio\n",
        "\n",
        "        elif method == 'wiener':\n",
        "            # Wiener filter approach\n",
        "            try:\n",
        "                from scipy import signal\n",
        "\n",
        "                # Estimate noise spectrum\n",
        "                noise_sample = audio[:int(0.5 * sr)]\n",
        "                f, Pxx = signal.welch(noise_sample, sr, nperseg=1024)\n",
        "\n",
        "                # Apply Wiener filter\n",
        "                audio_fft = np.fft.rfft(audio)\n",
        "                freq = np.fft.rfftfreq(len(audio), 1/sr)\n",
        "\n",
        "                # Interpolate noise PSD to match audio frequencies\n",
        "                noise_psd = np.interp(freq, f, Pxx, left=Pxx[0], right=Pxx[-1])\n",
        "\n",
        "                # Wiener filter\n",
        "                signal_psd = np.abs(audio_fft)**2\n",
        "                wiener_filter = signal_psd / (signal_psd + noise_psd)\n",
        "                audio_clean_fft = audio_fft * wiener_filter\n",
        "                audio_clean = np.fft.irfft(audio_clean_fft, n=len(audio))\n",
        "\n",
        "                return audio_clean\n",
        "            except:\n",
        "                return audio\n",
        "\n",
        "        else:\n",
        "            return audio\n",
        "\n",
        "    def _remove_silence(self, audio, sr, top_db=30, frame_length=2048,\n",
        "                       hop_length=512, ref=np.max):\n",
        "        \"\"\"\n",
        "        Remove leading and trailing silence\n",
        "\n",
        "        Args:\n",
        "            audio: Input audio array\n",
        "            sr: Sample rate\n",
        "            top_db: Threshold in dB below reference\n",
        "            frame_length: Length of the frame for STFT\n",
        "            hop_length: Hop length for STFT\n",
        "            ref: Reference power for dB calculation\n",
        "        \"\"\"\n",
        "        # Trim silence using librosa\n",
        "        audio_trimmed, _ = librosa.effects.trim(\n",
        "            audio,\n",
        "            top_db=top_db,\n",
        "            frame_length=frame_length,\n",
        "            hop_length=hop_length,\n",
        "            ref=ref\n",
        "        )\n",
        "\n",
        "        # If trimming removed everything, return original\n",
        "        if len(audio_trimmed) == 0:\n",
        "            return audio\n",
        "\n",
        "        return audio_trimmed\n",
        "\n",
        "    def _remove_silence_vad(self, audio, sr, aggressiveness=3):\n",
        "        \"\"\"\n",
        "        Remove silence using Voice Activity Detection (WebRTC VAD)\n",
        "\n",
        "        Args:\n",
        "            audio: Input audio array\n",
        "            sr: Sample rate\n",
        "            aggressiveness: VAD aggressiveness (0-3)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import webrtcvad\n",
        "            from scipy.io import wavfile\n",
        "\n",
        "            # VAD requires 16-bit PCM at 8kHz, 16kHz, 32kHz, or 48kHz\n",
        "            vad = webrtcvad.Vad(aggressiveness)\n",
        "\n",
        "            # Resample to 16kHz if needed\n",
        "            if sr != 16000:\n",
        "                audio = librosa.resample(y=audio, orig_sr=sr, target_sr=16000)\n",
        "                sr = 16000\n",
        "\n",
        "            # Convert to 16-bit PCM\n",
        "            audio_int16 = (audio * 32767).astype(np.int16)\n",
        "\n",
        "            # Frame parameters for VAD\n",
        "            frame_duration = 30  # ms\n",
        "            frame_size = int(sr * frame_duration / 1000)\n",
        "\n",
        "            # Split into frames\n",
        "            frames = []\n",
        "            for i in range(0, len(audio_int16), frame_size):\n",
        "                frame = audio_int16[i:i+frame_size]\n",
        "                if len(frame) < frame_size:\n",
        "                    # Pad last frame\n",
        "                    frame = np.pad(frame, (0, frame_size - len(frame)))\n",
        "                frames.append(frame)\n",
        "\n",
        "            # Detect speech frames\n",
        "            speech_frames = []\n",
        "            for frame in frames:\n",
        "                is_speech = vad.is_speech(frame.tobytes(), sr)\n",
        "                speech_frames.append(is_speech)\n",
        "\n",
        "            # Find speech segments\n",
        "            speech_segments = []\n",
        "            in_speech = False\n",
        "            start_idx = 0\n",
        "\n",
        "            for i, is_speech in enumerate(speech_frames):\n",
        "                if is_speech and not in_speech:\n",
        "                    start_idx = i * frame_size\n",
        "                    in_speech = True\n",
        "                elif not is_speech and in_speech:\n",
        "                    end_idx = i * frame_size\n",
        "                    speech_segments.append((start_idx, end_idx))\n",
        "                    in_speech = False\n",
        "\n",
        "            # If still in speech at end\n",
        "            if in_speech:\n",
        "                speech_segments.append((start_idx, len(audio_int16)))\n",
        "\n",
        "            # Extract speech segments\n",
        "            if speech_segments:\n",
        "                speech_audio = np.concatenate([\n",
        "                    audio_int16[start:end]\n",
        "                    for start, end in speech_segments\n",
        "                ])\n",
        "                # Convert back to float\n",
        "                speech_audio = speech_audio.astype(np.float32) / 32767\n",
        "            else:\n",
        "                speech_audio = audio_int16.astype(np.float32) / 32767\n",
        "\n",
        "            # Resample back to original rate if needed\n",
        "            if sr != self.target_sr:\n",
        "                speech_audio = librosa.resample(\n",
        "                    y=speech_audio,\n",
        "                    orig_sr=sr,\n",
        "                    target_sr=self.target_sr\n",
        "                )\n",
        "\n",
        "            return speech_audio\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Warning: webrtcvad not installed, using librosa trim\")\n",
        "            return self._remove_silence(audio, sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QZXQZqqyBy_"
      },
      "outputs": [],
      "source": [
        "# quick_process.py\n",
        "from pathlib import Path\n",
        "\n",
        "def quick_bulk_process(input_folder, output_folder=None):\n",
        "    \"\"\"\n",
        "    One-line bulk processing for common use cases\n",
        "    \"\"\"\n",
        "    if output_folder is None:\n",
        "        output_folder = Path(input_folder) / 'processed'\n",
        "\n",
        "    # Create processor\n",
        "    processor = AudioBulkProcessor(target_sr=16000, n_jobs=1)\n",
        "\n",
        "    # Process\n",
        "    results = processor.process_directory(\n",
        "        input_dir=input_folder,\n",
        "        output_dir=output_folder,\n",
        "        normalize=True,\n",
        "        noise_reduce=True,\n",
        "        remove_silence=True,\n",
        "        norm_params={'method': 'peak', 'target_level': -1.0},\n",
        "        noise_params={'method': 'spectral_gate', 'prop_decrease': 0.9},\n",
        "        silence_params={'top_db': 25}\n",
        "    )\n",
        "\n",
        "    print(f\"Processed {len([r for r in results if r[0]])} files\")\n",
        "    print(f\"Output saved to: {output_folder}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    quick_bulk_process('Emotion Speech Dataset/')\n",
        "    quick_bulk_process('TESS/')\n",
        "    quick_bulk_process('jl-corpus/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VpUWwhMxDfn"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQrWfQLExGW6"
      },
      "outputs": [],
      "source": [
        " # =====================\n",
        "# 1. SETUP AND CONFIGURATION\n",
        "# =====================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "SAMPLE_RATE = 22050\n",
        "N_MFCC = 13\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "TARGET_EMOTIONS = ['sad', 'happy', 'angry', 'surprise', 'neutral']\n",
        "\n",
        "# Paths to your datasets\n",
        "DATASET_PATHS = {\n",
        "    'jl-corpus': '/Users/omari/Master Classes/ML/jl-corpus/processed',\n",
        "    'TESS': '/Users/omari/Master Classes/ML/TESS/processed',\n",
        "    'ESD': '/Users/omari/Master Classes/ML/Emotion Speech Dataset/processed'\n",
        "}\n",
        "\n",
        "# =====================\n",
        "# 2. EMOTION MAPPING AND STANDARDIZATION\n",
        "# =====================\n",
        "\n",
        "# Comprehensive emotion mapping dictionary\n",
        "EMOTION_MAPPING = {\n",
        "    # Standard emotions (already correct)\n",
        "    'sad': 'sad',\n",
        "    'happy': 'happy',\n",
        "    'angry': 'angry',\n",
        "    'surprise': 'surprise',\n",
        "    'neutral': 'neutral',\n",
        "\n",
        "    # TESS specific variations\n",
        "    'pleasant_surprise': 'surprise',\n",
        "    'pleasant surprise': 'surprise',\n",
        "    'ps': 'surprise',  # TESS abbreviation\n",
        "    'fear': 'surprise',  # Map fear to surprise for consistency\n",
        "    'disgust': 'angry',  # Map disgust to angry\n",
        "    'fearful': 'surprise',\n",
        "    'disgusted': 'angry',\n",
        "\n",
        "    # Adjective/Noun variations\n",
        "    'sadness': 'sad',\n",
        "    'happiness': 'happy',\n",
        "    'anger': 'angry',\n",
        "    'surprised': 'surprise',\n",
        "\n",
        "    # ESD variations\n",
        "    'Surprise': 'surprise',\n",
        "    'Angry': 'angry',\n",
        "    'Happy': 'happy',\n",
        "    'Sad': 'sad',\n",
        "    'Neutral': 'neutral',\n",
        "\n",
        "    # JL Corpus variations\n",
        "    'SAD': 'sad',\n",
        "    'HAPPY': 'happy',\n",
        "    'ANGRY': 'angry',\n",
        "    'SURPRISE': 'surprise',\n",
        "    'NEUTRAL': 'neutral',\n",
        "\n",
        "    # Additional common variations\n",
        "    'joy': 'happy',\n",
        "    'joyful': 'happy',\n",
        "    'angryanger': 'angry',\n",
        "    'neutralneutral': 'neutral',\n",
        "}\n",
        "\n",
        "def standardize_emotion(input_emotion, filename, dataset_name):\n",
        "    \"\"\"\n",
        "    Standardize emotion names across all datasets\n",
        "    \"\"\"\n",
        "    if not input_emotion:\n",
        "        return None\n",
        "\n",
        "    input_lower = input_emotion.lower().strip().replace(' ', '_')\n",
        "\n",
        "    # Direct mapping first\n",
        "    if input_lower in EMOTION_MAPPING:\n",
        "        return EMOTION_MAPPING[input_lower]\n",
        "\n",
        "    # Check for partial matches\n",
        "    for key in EMOTION_MAPPING:\n",
        "        if key in input_lower:\n",
        "            return EMOTION_MAPPING[key]\n",
        "\n",
        "    # Dataset-specific heuristics\n",
        "    if dataset_name == 'TESS':\n",
        "        # TESS patterns: OAF_back_angry.wav or YAF_back_sad.wav\n",
        "        for emotion in TARGET_EMOTIONS:\n",
        "            if f'_{emotion}' in filename.lower() or f'-{emotion}' in filename.lower():\n",
        "                return emotion\n",
        "\n",
        "    elif dataset_name == 'jl-corpus':\n",
        "        # JL patterns: filename contains emotion\n",
        "        for emotion in TARGET_EMOTIONS:\n",
        "            if emotion in filename.lower():\n",
        "                return emotion\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_emotion_from_path(file_path, dataset_name):\n",
        "    \"\"\"\n",
        "    Extract emotion from file path based on dataset structure\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(file_path).lower()\n",
        "    parent_dir = os.path.basename(os.path.dirname(file_path)).lower()\n",
        "\n",
        "    emotion = None\n",
        "\n",
        "    # 1. JL Corpus: Emotion in filename\n",
        "    if dataset_name == 'jl-corpus':\n",
        "        for target_emotion in TARGET_EMOTIONS:\n",
        "            if target_emotion in filename:\n",
        "                emotion = target_emotion\n",
        "                break\n",
        "\n",
        "    # 2. ESD: Emotion in folder name\n",
        "    elif dataset_name == 'ESD':\n",
        "        # ESD structure: .../Angry/001/... or .../Surprise/002/...\n",
        "        for target_emotion in TARGET_EMOTIONS:\n",
        "            if target_emotion in parent_dir:\n",
        "                emotion = target_emotion\n",
        "                break\n",
        "\n",
        "    # 3. TESS: Both folder and filename contain emotion\n",
        "    elif dataset_name == 'TESS':\n",
        "        # Check folder first (TESS has emotion folders)\n",
        "        folder_emotions = []\n",
        "        for target_emotion in TARGET_EMOTIONS:\n",
        "            if target_emotion in parent_dir:\n",
        "                folder_emotions.append(target_emotion)\n",
        "\n",
        "        # Check filename\n",
        "        file_emotions = []\n",
        "        for target_emotion in TARGET_EMOTIONS:\n",
        "            if target_emotion in filename:\n",
        "                file_emotions.append(target_emotion)\n",
        "\n",
        "        # Prioritize folder emotion, fallback to filename\n",
        "        if folder_emotions:\n",
        "            emotion = folder_emotions[0]\n",
        "        elif file_emotions:\n",
        "            emotion = file_emotions[0]\n",
        "        else:\n",
        "            # Check for TESS specific patterns\n",
        "            if 'ps' in filename or 'pleasant' in filename:\n",
        "                emotion = 'surprise'\n",
        "            elif 'fear' in filename or parent_dir:\n",
        "                emotion = 'surprise'\n",
        "            elif 'disgust' in filename or parent_dir:\n",
        "                emotion = 'angry'\n",
        "\n",
        "    return emotion\n",
        "\n",
        "# =====================\n",
        "# 3. FEATURE EXTRACTION FUNCTIONS\n",
        "# =====================\n",
        "\n",
        "def extract_mfcc_features(audio_path, sr=SAMPLE_RATE, n_mfcc=N_MFCC):\n",
        "    \"\"\"\n",
        "    Extract MFCC features from audio file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio\n",
        "        audio, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
        "\n",
        "        # Extract MFCC features\n",
        "        mfccs = librosa.feature.mfcc(\n",
        "            y=audio,\n",
        "            sr=sr,\n",
        "            n_mfcc=n_mfcc,\n",
        "            n_fft=N_FFT,\n",
        "            hop_length=HOP_LENGTH\n",
        "        )\n",
        "\n",
        "        # Transpose to get (time_steps, features)\n",
        "        mfccs = mfccs.T\n",
        "\n",
        "        # Pad/trim to fixed length (3 seconds at target sample rate)\n",
        "        target_length = int(3 * sr / HOP_LENGTH)  # ~129 frames for 3s\n",
        "        if mfccs.shape[0] > target_length:\n",
        "            mfccs = mfccs[:target_length]\n",
        "        else:\n",
        "            padding = target_length - mfccs.shape[0]\n",
        "            mfccs = np.pad(mfccs, ((0, padding), (0, 0)), mode='constant')\n",
        "\n",
        "        return mfccs.astype(np.float32)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def scan_audio_files(dataset_paths):\n",
        "    \"\"\"\n",
        "    Scan all datasets and collect audio files with their standardized emotions\n",
        "    \"\"\"\n",
        "    all_files = []\n",
        "\n",
        "    for dataset_name, base_path in dataset_paths.items():\n",
        "        if not os.path.exists(base_path):\n",
        "            print(f\"‚ö†Ô∏è Dataset path not found: {base_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nüìÅ Scanning {dataset_name}...\")\n",
        "\n",
        "        # Collect all audio files\n",
        "        audio_extensions = ['.wav', '.mp3', '.flac', '.WAV', '.MP3']\n",
        "        audio_files = []\n",
        "\n",
        "        for root, dirs, files in os.walk(base_path):\n",
        "            for file in files:\n",
        "                if any(file.lower().endswith(ext) for ext in audio_extensions):\n",
        "                    audio_files.append(os.path.join(root, file))\n",
        "\n",
        "        print(f\"   Found {len(audio_files)} audio files\")\n",
        "\n",
        "        # Process each file\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for file_path in tqdm(audio_files, desc=f\"  Processing {dataset_name}\"):\n",
        "            # Extract emotion from path\n",
        "            raw_emotion = extract_emotion_from_path(file_path, dataset_name)\n",
        "\n",
        "            # Standardize emotion\n",
        "            standardized_emotion = standardize_emotion(\n",
        "                raw_emotion,\n",
        "                os.path.basename(file_path),\n",
        "                dataset_name\n",
        "            )\n",
        "\n",
        "            # Skip if emotion not in target list\n",
        "            if standardized_emotion not in TARGET_EMOTIONS:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            all_files.append({\n",
        "                'file_path': file_path,\n",
        "                'dataset': dataset_name,\n",
        "                'raw_emotion': raw_emotion,\n",
        "                'emotion': standardized_emotion,\n",
        "                'filename': os.path.basename(file_path)\n",
        "            })\n",
        "\n",
        "            processed_count += 1\n",
        "\n",
        "        print(f\"   ‚úÖ Processed: {processed_count}, ‚ùå Skipped: {skipped_count}\")\n",
        "\n",
        "    return all_files\n",
        "\n",
        "# =====================\n",
        "# 4. MAIN PROCESSING PIPELINE\n",
        "# =====================\n",
        "\n",
        "def create_unified_feature_file():\n",
        "    \"\"\"\n",
        "    Main function to create unified feature file\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üéµ UNIFIED FEATURE EXTRACTION PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Scan and categorize all files\n",
        "    print(\"\\nüìä Step 1: Scanning datasets and standardizing emotions...\")\n",
        "    all_files = scan_audio_files(DATASET_PATHS)\n",
        "\n",
        "    if not all_files:\n",
        "        print(\"‚ùå No valid audio files found!\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n‚úÖ Total valid files: {len(all_files)}\")\n",
        "\n",
        "    # Show emotion distribution\n",
        "    emotion_counts = {}\n",
        "    for file_info in all_files:\n",
        "        emotion = file_info['emotion']\n",
        "        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
        "\n",
        "    print(\"\\nüìà Emotion Distribution:\")\n",
        "    for emotion, count in sorted(emotion_counts.items()):\n",
        "        print(f\"  {emotion}: {count} files\")\n",
        "\n",
        "    # Step 2: Create HDF5 file for storage\n",
        "    print(\"\\nüìÅ Step 2: Creating unified feature file...\")\n",
        "\n",
        "    output_file = 'unified_audio_features.h5'\n",
        "\n",
        "    with h5py.File(output_file, 'w') as hf:\n",
        "        # Create datasets\n",
        "        features_dset = hf.create_dataset(\n",
        "            'features',\n",
        "            shape=(0, int(3 * SAMPLE_RATE / HOP_LENGTH), N_MFCC),\n",
        "            maxshape=(None, int(3 * SAMPLE_RATE / HOP_LENGTH), N_MFCC),\n",
        "            dtype=np.float32,\n",
        "            chunks=True,\n",
        "            compression='gzip'\n",
        "        )\n",
        "\n",
        "        # Create datasets for metadata\n",
        "        emotions_dset = hf.create_dataset(\n",
        "            'emotions',\n",
        "            shape=(0,),\n",
        "            maxshape=(None,),\n",
        "            dtype=h5py.string_dtype(encoding='utf-8')\n",
        "        )\n",
        "\n",
        "        filepaths_dset = hf.create_dataset(\n",
        "            'filepaths',\n",
        "            shape=(0,),\n",
        "            maxshape=(None,),\n",
        "            dtype=h5py.string_dtype(encoding='utf-8')\n",
        "        )\n",
        "\n",
        "        datasets_dset = hf.create_dataset(\n",
        "            'datasets',\n",
        "            shape=(0,),\n",
        "            maxshape=(None,),\n",
        "            dtype=h5py.string_dtype(encoding='utf-8')\n",
        "        )\n",
        "\n",
        "        # Step 3: Extract features and save\n",
        "        print(\"\\n‚ö° Step 3: Extracting MFCC features...\")\n",
        "\n",
        "        successful = 0\n",
        "        failed = 0\n",
        "\n",
        "        for idx, file_info in enumerate(tqdm(all_files, desc=\"Extracting features\")):\n",
        "            try:\n",
        "                # Extract features\n",
        "                features = extract_mfcc_features(file_info['file_path'])\n",
        "\n",
        "                if features is not None:\n",
        "                    # Resize datasets\n",
        "                    features_dset.resize(features_dset.shape[0] + 1, axis=0)\n",
        "                    emotions_dset.resize(emotions_dset.shape[0] + 1, axis=0)\n",
        "                    filepaths_dset.resize(filepaths_dset.shape[0] + 1, axis=0)\n",
        "                    datasets_dset.resize(datasets_dset.shape[0] + 1, axis=0)\n",
        "\n",
        "                    # Add data\n",
        "                    features_dset[-1] = features\n",
        "                    emotions_dset[-1] = file_info['emotion']\n",
        "                    filepaths_dset[-1] = file_info['file_path']\n",
        "                    datasets_dset[-1] = file_info['dataset']\n",
        "\n",
        "                    successful += 1\n",
        "                else:\n",
        "                    failed += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                failed += 1\n",
        "                continue\n",
        "\n",
        "        # Save emotion encoding mapping\n",
        "        emotion_to_idx = {emotion: i for i, emotion in enumerate(TARGET_EMOTIONS)}\n",
        "        hf.attrs['emotion_encoding'] = str(emotion_to_idx)\n",
        "        hf.attrs['sample_rate'] = SAMPLE_RATE\n",
        "        hf.attrs['n_mfcc'] = N_MFCC\n",
        "        hf.attrs['total_files'] = successful\n",
        "\n",
        "    print(f\"\\n‚úÖ Feature extraction complete!\")\n",
        "    print(f\"   Successful: {successful} files\")\n",
        "    print(f\"   Failed: {failed} files\")\n",
        "    print(f\"   Output file: {output_file}\")\n",
        "    print(f\"   File size: {os.path.getsize(output_file) / (1024**3):.2f} GB\")\n",
        "\n",
        "    # Create a summary CSV for easy reference\n",
        "    print(\"\\nüìã Creating summary CSV...\")\n",
        "    summary_data = []\n",
        "    with h5py.File(output_file, 'r') as hf:\n",
        "        for i in range(len(hf['emotions'])):\n",
        "            summary_data.append({\n",
        "                'filepath': hf['filepaths'][i].decode('utf-8'),\n",
        "                'dataset': hf['datasets'][i].decode('utf-8'),\n",
        "                'emotion': hf['emotions'][i].decode('utf-8'),\n",
        "                'feature_index': i\n",
        "            })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    summary_df.to_csv('features_summary.csv', index=False)\n",
        "\n",
        "    print(f\"‚úÖ Summary saved to: features_summary.csv\")\n",
        "\n",
        "    return output_file\n",
        "\n",
        "# =====================\n",
        "# 5. LOADING UTILITIES\n",
        "# =====================\n",
        "\n",
        "def load_features_from_h5(file_path, emotion_filter=None):\n",
        "    \"\"\"\n",
        "    Load features from HDF5 file with optional emotion filtering\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "    filepaths = []\n",
        "\n",
        "    with h5py.File(file_path, 'r') as hf:\n",
        "        total_files = hf.attrs['total_files']\n",
        "\n",
        "        for i in range(total_files):\n",
        "            emotion = hf['emotions'][i].decode('utf-8')\n",
        "\n",
        "            # Apply emotion filter if specified\n",
        "            if emotion_filter and emotion not in emotion_filter:\n",
        "                continue\n",
        "\n",
        "            features.append(hf['features'][i])\n",
        "            labels.append(emotion)\n",
        "            filepaths.append(hf['filepaths'][i].decode('utf-8'))\n",
        "\n",
        "    return np.array(features), np.array(labels), filepaths\n",
        "\n",
        "def get_dataset_info(file_path):\n",
        "    \"\"\"\n",
        "    Get information about the stored features\n",
        "    \"\"\"\n",
        "    with h5py.File(file_path, 'r') as hf:\n",
        "        print(\"üìä Feature File Information:\")\n",
        "        print(f\"   Total files: {hf.attrs['total_files']}\")\n",
        "        print(f\"   Sample rate: {hf.attrs['sample_rate']}\")\n",
        "        print(f\"   MFCC features: {hf.attrs['n_mfcc']}\")\n",
        "\n",
        "        # Count emotions\n",
        "        emotions = [e.decode('utf-8') for e in hf['emotions'][:]]\n",
        "        emotion_counts = {}\n",
        "        for emotion in emotions:\n",
        "            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
        "\n",
        "        print(\"\\nüé≠ Emotion Distribution:\")\n",
        "        for emotion, count in sorted(emotion_counts.items()):\n",
        "            print(f\"   {emotion}: {count} files\")\n",
        "\n",
        "        # Show dataset distribution\n",
        "        datasets = [d.decode('utf-8') for d in hf['datasets'][:]]\n",
        "        dataset_counts = {}\n",
        "        for dataset in datasets:\n",
        "            dataset_counts[dataset] = dataset_counts.get(dataset, 0) + 1\n",
        "\n",
        "        print(\"\\nüìÅ Dataset Distribution:\")\n",
        "        for dataset, count in sorted(dataset_counts.items()):\n",
        "            print(f\"   {dataset}: {count} files\")\n",
        "\n",
        "# =====================\n",
        "# 6. EXECUTION\n",
        "# =====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the feature extraction pipeline\n",
        "    feature_file = create_unified_feature_file()\n",
        "\n",
        "    if feature_file:\n",
        "        # Display information about the created file\n",
        "        get_dataset_info(feature_file)\n",
        "\n",
        "        # Example: Load all features for training\n",
        "        print(\"\\nüì• Example: Loading features for model training...\")\n",
        "        features, labels, filepaths = load_features_from_h5(feature_file)\n",
        "\n",
        "        print(f\"Loaded {len(features)} feature vectors\")\n",
        "        print(f\"Feature shape: {features[0].shape}\")\n",
        "        print(f\"Labels: {np.unique(labels, return_counts=True)}\")\n",
        "\n",
        "        # Download from Colab\n",
        "        print(f\"\\nüíæ To download in Colab:\")\n",
        "        print(\"from google.colab import files\")\n",
        "        print(\"files.download('unified_audio_features.h5')\")\n",
        "        print(\"files.download('features_summary.csv')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NQreTdlxIDg"
      },
      "source": [
        "# Data Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Kh3j_l1Fl6"
      },
      "source": [
        "## Basic CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31h68TNzxHpM"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# FIXED BASIC CNN TRAINING PIPELINE (Keras 3 + Mac M1/M2 Compatible)\n",
        "# =====================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# =====================\n",
        "# 1. LOAD DATA\n",
        "# =====================\n",
        "\n",
        "def load_real_data(features_file='unified_audio_features.h5'):\n",
        "    \"\"\"\n",
        "    Load actual MFCC features and labels from H5 file\n",
        "    \"\"\"\n",
        "    print(\"üì• Loading your real audio features...\")\n",
        "\n",
        "    if not os.path.exists(features_file):\n",
        "        raise FileNotFoundError(f\"‚ùå File {features_file} not found! Please run feature extraction first.\")\n",
        "\n",
        "    with h5py.File(features_file, 'r') as hf:\n",
        "        # Load features\n",
        "        features = hf['features'][:]\n",
        "        print(f\"   ‚úÖ Loaded {len(features)} feature matrices\")\n",
        "\n",
        "        # Load emotions\n",
        "        emotions = [e.decode('utf-8') for e in hf['emotions'][:]]\n",
        "        print(f\"   ‚úÖ Loaded {len(emotions)} emotion labels\")\n",
        "\n",
        "        # Load filepaths\n",
        "        filepaths = [f.decode('utf-8') for f in hf['filepaths'][:]]\n",
        "\n",
        "    # CNN REQUIREMENT: Shape must be (Batch, Height, Width, Channels)\n",
        "    # Current: (46424, 129, 13)\n",
        "    # Target:  (46424, 129, 13, 1)\n",
        "    features = np.expand_dims(features, axis=-1)\n",
        "    print(f\"   üìä Final Data Shape: {features.shape}\")\n",
        "\n",
        "    # Encode emotions\n",
        "    label_encoder = LabelEncoder()\n",
        "    emotion_labels = label_encoder.fit_transform(emotions)\n",
        "    onehot_labels = keras.utils.to_categorical(emotion_labels, 5)\n",
        "\n",
        "    # Print Distribution\n",
        "    print(f\"\\nüé≠ Emotion Distribution:\")\n",
        "    unique, counts = np.unique(emotions, return_counts=True)\n",
        "    for emotion, count in zip(unique, counts):\n",
        "        print(f\"   - {emotion}: {count}\")\n",
        "\n",
        "    return features, onehot_labels, label_encoder, filepaths\n",
        "\n",
        "# =====================\n",
        "# 2. DEFINE BASIC CNN MODEL (M1 FIXED)\n",
        "# =====================\n",
        "\n",
        "def create_basic_cnn_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Basic CNN Architecture adjusted for Mac M1/M2 stability.\n",
        "    REMOVED BatchNormalization layers to fix 'Incompatible shapes' error.\n",
        "    INCREASED Dropout to compensate for lack of BN regularization.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential(name=\"Basic_CNN_M1_Fixed\")\n",
        "\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "    # --- Conv Block 1 ---\n",
        "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "    # BatchNormalization REMOVED\n",
        "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "    # BatchNormalization REMOVED\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.25))  # Increased from 0.15\n",
        "\n",
        "    # --- Conv Block 2 ---\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    # BatchNormalization REMOVED\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    # BatchNormalization REMOVED\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.3))   # Increased from 0.2\n",
        "\n",
        "    # --- Conv Block 3 (Optional extra depth) ---\n",
        "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    # BatchNormalization REMOVED\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # --- Global Pooling ---\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    # --- Dense Layers ---\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    # BatchNormalization REMOVED\n",
        "    model.add(layers.Dropout(0.4))   # Increased from 0.3\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dropout(0.4))\n",
        "\n",
        "    # --- Output ---\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# =====================\n",
        "# 3. MAIN TRAINING PIPELINE\n",
        "# =====================\n",
        "\n",
        "def run_training_pipeline():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöÄ STARTING BASIC CNN TRAINING (Keras 3 Compatible)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. Load Data\n",
        "    try:\n",
        "        X, y, label_encoder, filepaths = load_real_data()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Critical Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Split Data (Stratified)\n",
        "    print(\"\\n‚úÇÔ∏è  Splitting data (80% Train, 20% Test)...\")\n",
        "    y_integers = np.argmax(y, axis=1)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_integers\n",
        "    )\n",
        "    print(f\"   Train shape: {X_train.shape}\")\n",
        "    print(f\"   Test shape:  {X_test.shape}\")\n",
        "\n",
        "    # 3. Create Model\n",
        "    print(\"\\nüèóÔ∏è  Creating Basic CNN Model...\")\n",
        "    model = create_basic_cnn_model(input_shape=X_train.shape[1:], num_classes=5)\n",
        "\n",
        "    # OPTIMIZER FIX: Use standard Adam (legacy is removed in Keras 3)\n",
        "    # Since we removed BatchNormalization, standard Adam is safe on M1.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "    print(\"   ‚úÖ Using Standard Adam Optimizer (Keras 3)\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    # 4. Class Weights\n",
        "    print(\"\\n‚öñÔ∏è  Calculating class weights...\")\n",
        "    train_labels_int = np.argmax(y_train, axis=1)\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=np.unique(train_labels_int),\n",
        "        y=train_labels_int\n",
        "    )\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # 5. Callbacks\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy', patience=12, restore_best_weights=True, verbose=1\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
        "        ),\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            'basic_cnn_best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1\n",
        "        ),\n",
        "        keras.callbacks.CSVLogger('training_history_cnn.csv')\n",
        "    ]\n",
        "\n",
        "    # 6. Train\n",
        "    print(\"\\nüî• Training Started...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=80,\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # =====================\n",
        "    # 4. FINAL EVALUATION\n",
        "    # =====================\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üìä FINAL EVALUATION\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Load Best Model\n",
        "    print(\"üì• Loading best saved model...\")\n",
        "    best_model = keras.models.load_model('basic_cnn_best_model.keras')\n",
        "\n",
        "    # Get Predictions\n",
        "    print(\"‚ö° Generating predictions on Test Set...\")\n",
        "    y_pred_probs = best_model.predict(X_test, verbose=1)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate Metrics\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='weighted')\n",
        "    rec = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\n‚úÖ Overall Accuracy:  {acc:.4f} ({acc*100:.2f}%)\")\n",
        "    print(f\"‚úÖ Weighted Precision: {prec:.4f}\")\n",
        "    print(f\"‚úÖ Weighted Recall:    {rec:.4f}\")\n",
        "    print(f\"‚úÖ Weighted F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    print(\"\\nüìã CLASSIFICATION REPORT\")\n",
        "    print(\"-\" * 60)\n",
        "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "    print(\"\\nüìâ CONFUSION MATRIX\")\n",
        "    print(\"-\" * 60)\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "    # Save Results\n",
        "    results_df = pd.DataFrame({\n",
        "        'filepath': [filepaths[i] for i in range(len(X_test))],\n",
        "        'true_emotion': label_encoder.inverse_transform(y_true),\n",
        "        'predicted_emotion': label_encoder.inverse_transform(y_pred),\n",
        "        'confidence': np.max(y_pred_probs, axis=1)\n",
        "    })\n",
        "    results_df.to_csv('basic_cnn_predictions.csv', index=False)\n",
        "    print(f\"\\nüíæ Predictions saved to 'basic_cnn_predictions.csv'\")\n",
        "\n",
        "    # Plot History\n",
        "    print(\"\\nüìà Plotting History...\")\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('cnn_training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_training_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB4-2h201PjF"
      },
      "source": [
        "## CRNN Model (CNN + LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rQ3E3le1ZFw"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# CRNN TRAINING PIPELINE FOR SER\n",
        "# =====================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =====================\n",
        "# 1. LOAD DATA\n",
        "# =====================\n",
        "\n",
        "def load_real_data(features_file='unified_audio_features.h5'):\n",
        "    \"\"\"\n",
        "    Load your actual 46K MFCC features and labels\n",
        "    \"\"\"\n",
        "    print(\"üì• Loading your real audio features...\")\n",
        "\n",
        "    with h5py.File(features_file, 'r') as hf:\n",
        "        # Load features\n",
        "        features = hf['features'][:]\n",
        "        print(f\"‚úÖ Loaded {len(features)} feature matrices\")\n",
        "\n",
        "        # Load emotions\n",
        "        emotions = [e.decode('utf-8') for e in hf['emotions'][:]]\n",
        "        print(f\"‚úÖ Loaded {len(emotions)} emotion labels\")\n",
        "\n",
        "        # Load filepaths for reference\n",
        "        filepaths = [f.decode('utf-8') for f in hf['filepaths'][:]]\n",
        "\n",
        "        # Check shapes\n",
        "        # Expected shape from H5: (N_samples, TimeSteps, N_MFCC) e.g., (46424, 129, 13)\n",
        "        print(f\"\\nüìä Data Shapes:\")\n",
        "        print(f\"  Features: {features.shape}\")\n",
        "\n",
        "    # CRNN requires shape: (Batch, Time, Features, Channels)\n",
        "    # The 'Time' dimension must be preserved for LSTM.\n",
        "    # Current: (N, 129, 13) -> Add channel: (N, 129, 13, 1)\n",
        "    features = np.expand_dims(features, axis=-1)\n",
        "    print(f\"  After adding channel: {features.shape}\")\n",
        "\n",
        "    # Encode emotions\n",
        "    label_encoder = LabelEncoder()\n",
        "    emotion_labels = label_encoder.fit_transform(emotions)\n",
        "\n",
        "    # One-hot encoding\n",
        "    onehot_labels = keras.utils.to_categorical(emotion_labels, 5)\n",
        "\n",
        "    print(f\"\\nüé≠ Emotion Distribution:\")\n",
        "    unique, counts = np.unique(emotions, return_counts=True)\n",
        "    for emotion, count in zip(unique, counts):\n",
        "        print(f\"  {emotion}: {count} samples\")\n",
        "\n",
        "    return features, onehot_labels, label_encoder, filepaths\n",
        "\n",
        "# =====================\n",
        "# 2. CREATE CRNN MODEL\n",
        "# =====================\n",
        "\n",
        "def create_crnn_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Creates a CRNN (Convolutional Recurrent Neural Network)\n",
        "    CNN extracts spatial features from MFCCs\n",
        "    LSTM captures temporal context of those features\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # --- CNN BLOCK (Feature Extractor) ---\n",
        "    # Input Shape: (129, 13, 1) -> (Time, Freq, Channel)\n",
        "\n",
        "    # Conv Block 1\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    # Pool only Frequency axis (axis 2), preserve Time (axis 1) slightly\n",
        "    # (2, 1) means downsample Time by 2, Frequency by 1 (keep frequency resolution high)\n",
        "    # OR standard (2,2) to reduce both. Let's use (2,2) for standard reduction.\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    # Conv Block 2\n",
        "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    # Conv Block 3\n",
        "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    # --- RECURRENT BLOCK (Temporal Context) ---\n",
        "\n",
        "    # Current Shape: (Batch, Reduced_Time, Reduced_Freq, Filters)\n",
        "    # We need to reshape for LSTM: (Batch, Time, Features)\n",
        "\n",
        "    # TimeDistributed(Flatten) collapses (Freq * Filters) into one vector per time step\n",
        "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
        "\n",
        "    # LSTM Layer\n",
        "    # Returns only the final state (return_sequences=False) for classification\n",
        "    model.add(layers.LSTM(128, return_sequences=False))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # --- CLASSIFICATION BLOCK ---\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# =====================\n",
        "# 3. TRAINING PIPELINE\n",
        "# =====================\n",
        "\n",
        "def train_on_real_data():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üéµ TRAINING CRNN (CNN + LSTM) ON REAL DATA\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. Load Data\n",
        "    X, y, label_encoder, filepaths = load_real_data()\n",
        "\n",
        "    # 2. Split Data\n",
        "    print(\"\\nüìä Step 2: Splitting data (80% train, 20% test)...\")\n",
        "    y_integers = np.argmax(y, axis=1)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y_integers\n",
        "    )\n",
        "\n",
        "    # 3. Create CRNN Model\n",
        "    print(\"\\nüèóÔ∏è  Step 3: Creating CRNN model...\")\n",
        "    model = create_crnn_model(input_shape=X_train.shape[1:], num_classes=5)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # 4. Class Weights\n",
        "    print(\"\\n‚öñÔ∏è  Step 4: Calculating class weights...\")\n",
        "    train_labels_int = np.argmax(y_train, axis=1)\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels_int), y=train_labels_int)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # 5. Callbacks\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=12, restore_best_weights=True, verbose=1),\n",
        "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "        keras.callbacks.ModelCheckpoint('crnn_best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "        keras.callbacks.CSVLogger('training_history_crnn.csv')\n",
        "    ]\n",
        "\n",
        "    # 6. Train\n",
        "    print(\"\\nüöÄ Step 6: Training CRNN...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=80,  # CRNNs might converge faster or need fewer epochs than pure CNNs\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 7. Evaluate\n",
        "    print(\"\\nüìä Step 7: Evaluating final model...\")\n",
        "    best_model = keras.models.load_model('crnn_best_model.keras')\n",
        "    test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # 8. Save Predictions\n",
        "    print(\"\\nüíæ Step 8: Saving predictions...\")\n",
        "    y_pred = np.argmax(best_model.predict(X_test, verbose=0), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'true_emotion': label_encoder.inverse_transform(y_true),\n",
        "        'predicted_emotion': label_encoder.inverse_transform(y_pred)\n",
        "    })\n",
        "    results_df.to_csv('crnn_predictions.csv', index=False)\n",
        "\n",
        "    # 9. Plot History\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val')\n",
        "    plt.title('CRNN Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train')\n",
        "    plt.plot(history.history['val_loss'], label='Val')\n",
        "    plt.title('CRNN Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('crnn_training_plot.png')\n",
        "    plt.show()\n",
        "\n",
        "    return best_model, history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_on_real_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRLD-sIj1vWR"
      },
      "source": [
        "## CNN + Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zXpzUnjQ_DcJ",
        "outputId": "cc442f84-8197-4bb7-db05-cf11c702a8c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading data from unified_audio_features.h5...\n",
            "Error loading data: ‚ùå Error: File 'unified_audio_features.h5' not found.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# STRATEGY C: REGULARIZED GRU + ROBUST SPECAUGMENT\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers, backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import h5py\n",
        "\n",
        "# ==========================================\n",
        "# 1. DATA LOADING\n",
        "# ==========================================\n",
        "def load_real_data(features_file='unified_audio_features.h5'):\n",
        "    print(f\"üì• Loading data from {features_file}...\")\n",
        "\n",
        "    if not os.path.exists(features_file):\n",
        "        raise FileNotFoundError(f\"‚ùå Error: File '{features_file}' not found.\")\n",
        "\n",
        "    with h5py.File(features_file, 'r') as hf:\n",
        "        features = hf['features'][:]\n",
        "        emotions = [e.decode('utf-8') for e in hf['emotions'][:]]\n",
        "\n",
        "    # Expand dims for channel (e.g., 128, 128) -> (128, 128, 1)\n",
        "    features = np.expand_dims(features, axis=-1)\n",
        "\n",
        "    # Re-encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    emotion_labels = label_encoder.fit_transform(emotions)\n",
        "    num_classes = len(np.unique(emotion_labels))\n",
        "    onehot_labels = keras.utils.to_categorical(emotion_labels, num_classes)\n",
        "\n",
        "    print(f\"‚úÖ Data Loaded: {features.shape} samples\")\n",
        "    return features, onehot_labels, label_encoder, num_classes\n",
        "\n",
        "# ==========================================\n",
        "# 2. ROBUST SPECAUGMENT IMPLEMENTATION\n",
        "# ==========================================\n",
        "def spec_augment(spectrogram, num_freq_masks=2, num_time_masks=2, freq_mask_param=15, time_mask_param=20):\n",
        "    \"\"\"\n",
        "    Applies SpecAugment with safety checks for small dimensions.\n",
        "    \"\"\"\n",
        "    aug_spectrogram = tf.identity(spectrogram)\n",
        "    shape = tf.shape(aug_spectrogram)\n",
        "    height = shape[0]\n",
        "    width = shape[1]\n",
        "\n",
        "    # Cast params to int32\n",
        "    freq_mask_param = tf.cast(freq_mask_param, tf.int32)\n",
        "    time_mask_param = tf.cast(time_mask_param, tf.int32)\n",
        "\n",
        "    # --- Frequency Masking ---\n",
        "    for _ in range(num_freq_masks):\n",
        "        # üõ°Ô∏è SAFETY FIX: Ensure mask height is never larger than image height - 1\n",
        "        # If image is 13px high, mask can be at most 12px.\n",
        "        safe_f_max = tf.maximum(1, tf.minimum(freq_mask_param, height - 1))\n",
        "\n",
        "        f = tf.random.uniform([], minval=0, maxval=safe_f_max, dtype=tf.int32)\n",
        "        f0 = tf.random.uniform([], minval=0, maxval=height - f, dtype=tf.int32)\n",
        "\n",
        "        mask_start = f0\n",
        "        mask_end = f0 + f\n",
        "        indices = tf.range(height)\n",
        "        mask = (indices < mask_start) | (indices >= mask_end)\n",
        "        mask = tf.reshape(mask, (height, 1, 1))\n",
        "        mask = tf.cast(mask, aug_spectrogram.dtype)\n",
        "        aug_spectrogram = aug_spectrogram * mask\n",
        "\n",
        "    # --- Time Masking ---\n",
        "    for _ in range(num_time_masks):\n",
        "        # üõ°Ô∏è SAFETY FIX: Ensure mask width is never larger than image width - 1\n",
        "        safe_t_max = tf.maximum(1, tf.minimum(time_mask_param, width - 1))\n",
        "\n",
        "        t = tf.random.uniform([], minval=0, maxval=safe_t_max, dtype=tf.int32)\n",
        "        t0 = tf.random.uniform([], minval=0, maxval=width - t, dtype=tf.int32)\n",
        "\n",
        "        mask_start = t0\n",
        "        mask_end = t0 + t\n",
        "        indices = tf.range(width)\n",
        "        mask = (indices < mask_start) | (indices >= mask_end)\n",
        "        mask = tf.reshape(mask, (1, width, 1))\n",
        "        mask = tf.cast(mask, aug_spectrogram.dtype)\n",
        "        aug_spectrogram = aug_spectrogram * mask\n",
        "\n",
        "    return aug_spectrogram\n",
        "\n",
        "def apply_spec_augment_wrapper(image, label):\n",
        "    # Wrapper for tf.data pipeline\n",
        "    return spec_augment(image), label\n",
        "\n",
        "# ==========================================\n",
        "# 3. REGULARIZED CRNN ARCHITECTURE\n",
        "# ==========================================\n",
        "def build_regularized_crnn(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # --- CNN Block (Feature Extraction) ---\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # --- Bridge to RNN ---\n",
        "    # Permute to (Batch, Time, Freq, Channels)\n",
        "    x = layers.Permute((2, 1, 3))(x)\n",
        "\n",
        "    # Reshape: Combine Frequency and Channels into 'Features'\n",
        "    target_shape = (-1, x.shape[2] * x.shape[3])\n",
        "    x = layers.Reshape(target_shape=(x.shape[1], x.shape[2] * x.shape[3]))(x)\n",
        "\n",
        "    # --- RNN Block (Regularized GRU) ---\n",
        "    x = layers.Bidirectional(\n",
        "        layers.GRU(64, return_sequences=True, dropout=0.4, recurrent_dropout=0.0)\n",
        "    )(x)\n",
        "\n",
        "    x = layers.Bidirectional(\n",
        "        layers.GRU(64, return_sequences=False, dropout=0.4, recurrent_dropout=0.0)\n",
        "    )(x)\n",
        "\n",
        "    # --- Output Block ---\n",
        "    x = layers.Dense(64, kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name=\"Regularized_CRNN_GRU\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# 4. EXECUTION PIPELINE\n",
        "# ==========================================\n",
        "def main_training_pipeline():\n",
        "    # A. Load Data\n",
        "    try:\n",
        "        X, y, label_encoder, num_classes = load_real_data()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    # B. Split Data\n",
        "    y_integers = np.argmax(y, axis=1)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y_integers\n",
        "    )\n",
        "\n",
        "    print(f\"üîπ Training Set: {X_train.shape}\")\n",
        "    print(f\"üîπ Test Set:     {X_test.shape}\")\n",
        "\n",
        "    # C. Create TF Datasets\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # Train Dataset (With Safe Augmentation)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    train_ds = train_ds.shuffle(buffer_size=1000)\n",
        "    train_ds = train_ds.map(apply_spec_augment_wrapper, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Test Dataset (No Augmentation)\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # D. Build Model\n",
        "    input_shape = X_train.shape[1:]\n",
        "    model = build_regularized_crnn(input_shape, num_classes)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # E. Callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint('crnn_best_model_regularized.keras', save_best_only=True, monitor='val_accuracy'),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, min_lr=1e-6),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=12, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    # F. Train\n",
        "    print(\"\\nüöÄ Starting Training (Regularized GRU + SpecAugment)...\")\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=callbacks)\n",
        "\n",
        "    # ==========================================\n",
        "    # 5. DETAILED EVALUATION\n",
        "    # ==========================================\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üìä FINAL EVALUATION METRICS\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Load best weights\n",
        "    model.load_weights('crnn_best_model_regularized.keras')\n",
        "\n",
        "    # Generate Predictions\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='weighted')\n",
        "    rec = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"‚úÖ Overall Accuracy:  {acc:.4f} ({acc*100:.2f}%)\")\n",
        "    print(f\"‚úÖ Weighted Precision: {prec:.4f}\")\n",
        "    print(f\"‚úÖ Weighted Recall:    {rec:.4f}\")\n",
        "    print(f\"‚úÖ Weighted F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    print(\"\\nüìã CLASSIFICATION REPORT\")\n",
        "    print(\"-\" * 60)\n",
        "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "    plt.title('Confusion Matrix - Regularized CRNN')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig('confusion_matrix_regularized.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Training History\n",
        "    if history is not None:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "        plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "        plt.title('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Train Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "        plt.title('Loss')\n",
        "        plt.legend()\n",
        "        plt.savefig('training_history_regularized.png')\n",
        "        plt.show()\n",
        "\n",
        "    # Save Predictions CSV\n",
        "    results_df = pd.DataFrame({\n",
        "        'true_emotion': label_encoder.inverse_transform(y_true),\n",
        "        'predicted_emotion': label_encoder.inverse_transform(y_pred),\n",
        "        'confidence': np.max(y_pred_probs, axis=1)\n",
        "    })\n",
        "    results_df.to_csv('crnn_regularized_predictions.csv', index=False)\n",
        "    print(f\"\\nüíæ Results saved to 'crnn_regularized_predictions.csv'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_training_pipeline()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}